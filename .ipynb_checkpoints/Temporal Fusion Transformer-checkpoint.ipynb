{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "703731b9",
   "metadata": {},
   "source": [
    "# Temporal Fusion Transformer\n",
    "\n",
    "- Notebook author: Kalle Bylin\n",
    "\n",
    "Based on the paper *Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting* by Bryan Lima, Sercan O. ArÄ±k, Nicolas Loeff and Tomas Pfister\n",
    "\n",
    "Source: https://arxiv.org/pdf/1912.09363.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc6b940",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51da2790",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861d036e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8efd5a2",
   "metadata": {},
   "source": [
    "## Basic Components\n",
    "\n",
    "The Temporal Fusion Transfomer architecture is composed of multiple components. We will start by building these components individually so that we then can use them on different types of problems.\n",
    "\n",
    "\n",
    "The final TFT architecture defined in the paper *Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting* has the following structure:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033fab2f",
   "metadata": {},
   "source": [
    "![TFT](img/tft_architecture.png)\n",
    "\n",
    "Source: https://arxiv.org/pdf/1912.09363.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653e31d6",
   "metadata": {},
   "source": [
    "### Gated Linear Unit & Gated Residual Network\n",
    "\n",
    "Gated Residual Network blocks are among the main basic components of this network. They enable efficient information flow along with the skip connections and gating layers.\n",
    "\n",
    "The gating mechanisms basically allow the network to adapt both depth and complexity in order to perform well on a wide range of datasets and tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a394baa",
   "metadata": {},
   "source": [
    "**Gated Linear Unit**\n",
    "\n",
    "It is hard to know which variables are actually relevant for the prediction task from the outset. The gates of the Gated Linear Unit make it possible to suppress parts of the architecture that are not necessary in a particular scneario or with a specific dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d946a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLU(nn.Module):\n",
    "    \"\"\"\n",
    "      The Gated Linear Unit GLU(a,b) = mult(a,sigmoid(b)) is common in NLP \n",
    "      architectures like the Gated CNN. Here sigmoid(b) corresponds to a gate \n",
    "      that controls what information from a is passed to the following layer. \n",
    "\n",
    "      Args:\n",
    "          input_size (int): number defining input and output size of the gate\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input\n",
    "        self.a = nn.Linear(input_size, input_size)\n",
    "\n",
    "        # Gate\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.b = nn.Linear(input_size, input_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.tensor): tensor passing through the gate\n",
    "        \"\"\"\n",
    "        gate = self.sigmoid(self.b(x))\n",
    "        x = self.a(x)\n",
    "        \n",
    "        return torch.mul(gate, x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99ab303",
   "metadata": {},
   "source": [
    "**Temporal Layer**\n",
    "\n",
    "Keras has a TimeDistributed layer wrapper that makes it possible to apply a layer to every temporal slice of an input.\n",
    "\n",
    "For example, it can be used to apply the same instance of a convolutional layer with the same set of weights on each timestep in the data. \n",
    "\n",
    "This TemporalLayer tries to reproduce this same functionality in Pytorch by collapsing the tensor before passing it through the layer and then rebuilding the original shape before returning the resulting tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5b65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalLayer(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Collapses input of dim T*N*H to (T*N)*H, and applies to a module.\n",
    "        Allows handling of variable sequence lengths and minibatch sizes.\n",
    "\n",
    "        Similar to TimeDistributed in Keras, it is a wrapper that makes it possible\n",
    "        to apply a layer to every temporal slice of an input.\n",
    "        \"\"\"\n",
    "        self.module = module\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.tensor): tensor with time steps to pass through the same layer.\n",
    "        \"\"\"\n",
    "        t, n = x.size(0), x.size(1)\n",
    "        x = x.reshape(t * n, -1)\n",
    "        x = self.module(x)\n",
    "        x = x.reshape(t, n, x.size(-1))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603e026b",
   "metadata": {},
   "source": [
    "**Gated Residual Network**\n",
    "\n",
    "The Gated Residual Network is a flexible block that can apply non-linear processing when required. The Gated Linear Unit defined above helps the GRN how much to contribute to its input and could potentially skip the layer altogether if necessary. GLU outputs close to 0 would supreess the non-linear contribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8df1adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedResidualNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "      The Gated Residual Network gives the model flexibility to apply non-linear\n",
    "      processing only when needed. It is difficult to know beforehand which\n",
    "      variables are relevant and in some cases simpler models can be beneficial.\n",
    "\n",
    "      GRN(a, c) = LayerNorm(a + GLU(eta_1)) # Dropout is applied to eta_1\n",
    "        eta_1 = W_1*eta_2 + b_1\n",
    "        eta_2 = ELU(W_2*a + W_3*c + b_2)\n",
    "      \n",
    "      Args:\n",
    "          input_size (int): Size of the input\n",
    "          hidden_size (int): Size of the hidden layer\n",
    "          output_size (int): Size of the output layer\n",
    "          dropout (float): Fraction between 0 and 1 corresponding to the degree of dropout used\n",
    "          context_size (int): Size of the static context vector\n",
    "          is_temporal (bool): Flag to decide if TemporalLayer has to be used or not\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout, context_size=None, is_temporal=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.context_size = context_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        self.is_temporal = is_temporal\n",
    "        \n",
    "        if self.is_temporal:\n",
    "            if self.input_size != self.output_size:\n",
    "                self.skip_layer = TemporalLayer(nn.Linear(self.input_size, self.output_size))\n",
    "\n",
    "            # Context vector c\n",
    "            if self.context_size != None:\n",
    "                self.c = TemporalLayer(nn.Linear(self.context_size, self.hidden_size, bias=False))\n",
    "\n",
    "            # Dense & ELU\n",
    "            self.dense1 = TemporalLayer(nn.Linear(self.input_size, self.hidden_size))\n",
    "            self.elu = nn.ELU()\n",
    "\n",
    "            # Dense & Dropout\n",
    "            self.dense2 = TemporalLayer(nn.Linear(self.hidden_size,  self.output_size))\n",
    "            self.dropout = nn.Dropout(self.dropout)\n",
    "\n",
    "            # Gate, Add & Norm\n",
    "            self.gate = TemporalLayer(GLU(self.output_size))\n",
    "            self.layer_norm = TemporalLayer(nn.BatchNorm1d(self.output_size))\n",
    "\n",
    "        else:\n",
    "            if self.input_size != self.output_size:\n",
    "                self.skip_layer = nn.Linear(self.input_size, self.output_size)\n",
    "\n",
    "            # Context vector c\n",
    "            if self.context_size != None:\n",
    "                self.c = nn.Linear(self.context_size, self.hidden_size, bias=False)\n",
    "\n",
    "            # Dense & ELU\n",
    "            self.dense1 = nn.Linear(self.input_size, self.hidden_size)\n",
    "            self.elu = nn.ELU()\n",
    "\n",
    "            # Dense & Dropout\n",
    "            self.dense2 = nn.Linear(self.hidden_size,  self.output_size)\n",
    "            self.dropout = nn.Dropout(self.dropout)\n",
    "\n",
    "            # Gate, Add & Norm\n",
    "            self.gate = GLU(self.output_size)\n",
    "            self.layer_norm = nn.BatchNorm1d(self.output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, c=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.tensor): tensor thas passes through the GRN\n",
    "            c (torch.tensor): Optional static context vector\n",
    "        \"\"\"\n",
    "\n",
    "        if self.input_size!=self.output_size:\n",
    "            a = self.skip_layer(x)\n",
    "        else:\n",
    "            a = x\n",
    "        \n",
    "        x = self.dense1(x)\n",
    "\n",
    "        if c != None:\n",
    "            c = self.c(c.unsqueeze(1))\n",
    "            x += c\n",
    "\n",
    "        eta_2 = self.elu(x)\n",
    "        \n",
    "        eta_1 = self.dense2(eta_2)\n",
    "        eta_1 = self.dropout(eta_1)\n",
    "\n",
    "        gate = self.gate(eta_1)\n",
    "        gate += a\n",
    "        x = self.layer_norm(gate)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b35796",
   "metadata": {},
   "source": [
    "### Variable Selection Network\n",
    "\n",
    "The Variable Selection Network is a critical component of the TFT architecture. This model accepts a wide variety of inputs as can be seen in this image from the paper:\n",
    "\n",
    "![TFT inputs](img/tft_inputs.png)\n",
    "\n",
    "\n",
    "Observed inputs are time dependent variables that are known only up until the moment when we want to forecast the target variable (this includes past values of the target variable).\n",
    "\n",
    "Known inputs are time dependent variables that can be known ahead of time (e.g. holidays, special events, etc.)\n",
    "\n",
    "Static covariates can also be used to enrich the model (e.g. region of a store).\n",
    "\n",
    "With so many variables we might end up with unnecessary noise that can have a negative impact on the performance of the model. The Variable Selection Network makes it possible for the model to eliminate this noise.\n",
    "\n",
    "The Variable Selection Network can then also be used to evaluate which variables are most important for the prediction task. This is critical for interpretability of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972f819a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariableSelectionNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "      The Variable Selection Network gives the model the ability to remove\n",
    "      unnecessary noisy inputs that could have a negative impact on performance.\n",
    "      It also allows us to better understand which variables are most important\n",
    "      for the prediction task.\n",
    "\n",
    "      The variable selection weights are created by feeding both the flattened\n",
    "      vector of all past inputs at time t (E_t) and an optional context vector \n",
    "      through a GRN, followed by a Softmax layer.\n",
    "\n",
    "      V_xt = Softmax(GRN_v(E_t, c_s)) \n",
    "\n",
    "      Also, the feature vector for each variable is fed through its \n",
    "      own GRN to create an additional layer of non-linear processing.\n",
    "\n",
    "      Processed features are then weighted by the variable selection weights\n",
    "      and combined.\n",
    "\n",
    "      Args:\n",
    "          input_size (int): Size of the input\n",
    "          output_size (int): Size of the output layer\n",
    "          hidden_size (int): Size of the hidden layer\n",
    "          dropout (float): Fraction between 0 and 1 corresponding to the degree of dropout used\n",
    "          context_size (int): Size of the static context vector\n",
    "          is_temporal (bool): Flag to decide if TemporalLayer has to be used or not\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size, hidden_size, dropout, context_size=None, is_temporal=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout = dropout\n",
    "        self.context_size = context_size\n",
    "        self.is_temporal = is_temporal\n",
    "       \n",
    "        self.flattened_inputs = GatedResidualNetwork(self.output_size*self.input_size, \n",
    "                                                     self.hidden_size, self.output_size, \n",
    "                                                     self.dropout, self.context_size, \n",
    "                                                     self.is_temporal)\n",
    "        \n",
    "        self.transformed_inputs = nn.ModuleList(\n",
    "            [GatedResidualNetwork(\n",
    "                self.input_size, self.hidden_size, self.hidden_size, \n",
    "                self.dropout, self.context_size, self.is_temporal) for i in range(self.output_size)])\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "\n",
    "    def forward(self, embedding, context=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          embedding (torch.tensor): Entity embeddings for categorical variables and linear \n",
    "                     transformations for continuous variables.\n",
    "          context (torch.tensor): The context is obtained from a static covariate encoder and\n",
    "                   is naturally omitted for static variables as they already\n",
    "                   have access to this\n",
    "        \"\"\"\n",
    "\n",
    "        # Generation of variable selection weights\n",
    "        sparse_weights = self.flattened_inputs(embedding, context)\n",
    "        if self.is_temporal:\n",
    "            sparse_weights = self.softmax(sparse_weights).unsqueeze(2)\n",
    "        else:\n",
    "            sparse_weights = self.softmax(sparse_weights).unsqueeze(1)\n",
    "\n",
    "        # Additional non-linear processing for each feature vector\n",
    "        transformed_embeddings = torch.stack(\n",
    "            [self.transformed_inputs[i](embedding[\n",
    "                Ellipsis, i*self.input_size:(i+1)*self.input_size]) for i in range(self.output_size)], axis=-1)\n",
    "\n",
    "        # Processed features are weighted by their corresponding weights and combined\n",
    "        combined = transformed_embeddings*sparse_weights\n",
    "        combined = combined.sum(axis=-1)\n",
    "\n",
    "        return combined, sparse_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84e46f2",
   "metadata": {},
   "source": [
    "### Interpretable Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5374cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "\n",
    "    def __init__(self:\n",
    "        pass\n",
    "        \n",
    "\n",
    "    def forward(self):\n",
    "        pass\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985f2702",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpretableMultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9358d3",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d1d8d3",
   "metadata": {},
   "source": [
    "The dataset code is decoupled from our model training code for better readability and modularity.\n",
    "\n",
    "As described in Pytorch documentation, a custom Dataset class must implement three functions: \\_\\_init__, \\_\\_len__, and \\_\\_getitem__ .\n",
    "\n",
    "Source: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570f662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFT_Dataset(Dataset):\n",
    "    def __init__(self, data, entity_column, time_column, target_column, \n",
    "                 input_columns, encoder_steps, decoder_steps):\n",
    "        \"\"\"\n",
    "          data (pd.DataFrame): dataframe containing raw data\n",
    "          entity_column (str): name of column containing entity data\n",
    "          time_column (str): name of column containing date data\n",
    "          target_column (str): name of column we need to predict\n",
    "          input_columns (list): list of string names of columns used as input\n",
    "          encoder_steps (int): number of known past time steps used for forecast. Equivalent to size of LSTM encoder\n",
    "          decoder_steps (int): number of input time steps used for each forecast date. Equivalent to the width N of the decoder\n",
    "        \"\"\"\n",
    "        \n",
    "        self.encoder_steps = encoder_steps\n",
    "        self.transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "        \n",
    "        \n",
    "        inputs = []\n",
    "        outputs = []\n",
    "        entity = []\n",
    "        time = []\n",
    "\n",
    "        for _, entity_group in data.groupby(entity_column):\n",
    "            \n",
    "            data_time_steps = len(entity_group)\n",
    "\n",
    "            if data_time_steps >= decoder_steps:\n",
    "                x = entity_group[input_columns].copy().values\n",
    "                inputs.append(\n",
    "                    np.stack([x[i:data_time_steps - (decoder_steps - 1) + i, :] for i in range(decoder_steps)], axis=1))\n",
    "\n",
    "                y = entity_group[[target_column]].copy().values\n",
    "                outputs.append(\n",
    "                    np.stack([y[i:data_time_steps - (decoder_steps - 1) + i, :] for i in range(decoder_steps)], axis=1))\n",
    "\n",
    "                e = entity_group[[entity_column]].copy().values\n",
    "                entity.append(\n",
    "                    np.stack([e[i:data_time_steps - (decoder_steps - 1) + i, :] for i in range(decoder_steps)], axis=1))\n",
    "\n",
    "                t = entity_group[[time_column]].copy().values\n",
    "                time.append(\n",
    "                    np.stack([t[i:data_time_steps - (decoder_steps - 1) + i, :] for i in range(decoder_steps)], axis=1))\n",
    "\n",
    "            else:\n",
    "                inputs.append(None)\n",
    "                outputs.append(None)\n",
    "                entity.append(None)\n",
    "                time.append(None)\n",
    "\n",
    "        self.inputs = np.concatenate(inputs, axis=0)\n",
    "        self.outputs = np.concatenate(outputs, axis=0)[:, encoder_steps:, :]\n",
    "        self.entity = np.concatenate(entity, axis=0)\n",
    "        #self.time = np.concatenate(time, axis=0)\n",
    "        self.active_inputs = np.ones_like(outputs)\n",
    "\n",
    "        self.sampled_data = {\n",
    "            'inputs': self.inputs,\n",
    "            'outputs': self.outputs[:, self.encoder_steps:, :],\n",
    "            'active_entries': np.ones_like(self.outputs[:, self.encoder_steps:, :]),\n",
    "            #'time': self.time,\n",
    "            'identifier': self.entity\n",
    "        }\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        s = {\n",
    "        'inputs': self.inputs[index],\n",
    "        'outputs': self.outputs[index], \n",
    "        'active_entries': np.ones_like(self.outputs[index]), \n",
    "        #'time': self.time[index],\n",
    "        'identifier': self.entity[index]\n",
    "        }\n",
    "\n",
    "        return s\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.inputs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bbb7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_preprocessing(train, real_columns, categorical_columns):\n",
    "    real_scalers = StandardScaler().fit(train[real_columns].values)\n",
    "\n",
    "    categorical_scalers = {}\n",
    "    num_classes = []\n",
    "    for col in categorical_columns:\n",
    "        srs = train[col].apply(str) \n",
    "        categorical_scalers[col] = LabelEncoder().fit(srs.values)\n",
    "        num_classes.append(srs.nunique())\n",
    "\n",
    "    return real_scalers, categorical_scalers\n",
    "\n",
    "\n",
    "def transform_inputs(df, real_scalers, categorical_scalers, real_columns, categorical_columns):\n",
    "    out = df.copy()\n",
    "    out[real_columns] = real_scalers.transform(df[real_columns].values)\n",
    "\n",
    "    for col in categorical_columns:\n",
    "        string_df = df[col].apply(str)\n",
    "        out[col] = categorical_scalers[col].transform(string_df)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f354b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('../formatted_omi_vol.csv', index_col=0)\n",
    "\n",
    "train = raw_data[raw_data['year'] < 2016]\n",
    "valid = raw_data.loc[(raw_data['year'] >= 2016) & (raw_data['year'] < 2018)]\n",
    "test = raw_data.loc[(raw_data['year'] >= 2018) & (raw_data.index <= '2019-06-28')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2bf66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca012482",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_columns = ['log_vol', 'open_to_close', 'days_from_start']\n",
    "categorical_columns = ['Symbol', 'day_of_week', 'day_of_month', 'week_of_year', 'month', 'Region']\n",
    "\n",
    "real_scalers, categorical_scalers = fit_preprocessing(train, real_columns, categorical_columns)\n",
    "\n",
    "train = transform_inputs(train, real_scalers, categorical_scalers, real_columns, categorical_columns)\n",
    "valid = transform_inputs(valid, real_scalers, categorical_scalers, real_columns, categorical_columns)\n",
    "test = transform_inputs(test, real_scalers, categorical_scalers, real_columns, categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c24a529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "DROPOUT = 0.3\n",
    "ENCODER_STEPS = 252\n",
    "DECODER_STEPS = 252 + 5\n",
    "HIDDEN_LAYER_SIZE = 160\n",
    "EMBEDDING_DIMENSION = 8\n",
    "QUANTILES = [0.1, 0.5, 0.9]\n",
    "\n",
    "\n",
    "# Dataset variables\n",
    "target_column = \"log_vol\"\n",
    "entity_column = \"Symbol\"\n",
    "time_column = \"date\"\n",
    "input_columns = [\"log_vol\", \"open_to_close\", \"days_from_start\", \"day_of_week\", \n",
    "                 \"day_of_month\", \"week_of_year\", \"month\", \"Region\", \"Symbol\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c76c6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"quantiles\": QUANTILES,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"dropout\": DROPOUT,\n",
    "    \"hidden_layer_size\": HIDDEN_LAYER_SIZE,\n",
    "    \"embedding_dim\": EMBEDDING_DIMENSION,\n",
    "    \"col_to_idx\": {col: idx for idx, col in enumerate(input_columns)},\n",
    "    \"static_covariates\": [\"Region\", \"Symbol\"],\n",
    "    \"category_counts\": {\"day_of_week\": 7, \"day_of_month\": 31, \"week_of_year\": 53, \"month\": 12, \"Region\": 4, \"Symbol\": 31},\n",
    "    \"device\": \"cpu\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d77baaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = TFT_Dataset(train, entity_column, time_column, target_column, input_columns, ENCODER_STEPS, DECODER_STEPS)\n",
    "validation_data = TFT_Dataset(valid, entity_column, time_column, target_column, input_columns, ENCODER_STEPS, DECODER_STEPS)\n",
    "testing_data = TFT_Dataset(test, entity_column, time_column, target_column, input_columns, ENCODER_STEPS, DECODER_STEPS)\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, num_workers=2, shuffle=False)\n",
    "valid_dataloader = DataLoader(validation_data, batch_size=BATCH_SIZE, num_workers=2, shuffle=False)\n",
    "test_dataloader = DataLoader(testing_data, batch_size=BATCH_SIZE, num_workers=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c044d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TemporalFusionTransformer(nn.Module):\n",
    "    \"\"\"Creates a Temporal Fusion Transformer model.\n",
    "\n",
    "    For simplicity, arguments are passed within a parameters dictionary\n",
    "\n",
    "    Args:\n",
    "        col_to_idx (dict): Maps column names to their index in input array\n",
    "        static_covariates (list): Names of static covariate variables\n",
    "        category_counts (dict): Maps column names to the number of categories of each categorical feature\n",
    "        batch_size (int): Batch size\n",
    "        hidden_size (int): Internal state size of different layers \n",
    "        dropout (float): Fraction between 0 and 1 corresponding to the degree of dropout used\n",
    "        embedding_dim (int): Dimensionality of embeddings\n",
    "        quantiles (list): Quantiles used for prediction. Also defines model output size\n",
    "        device (str): Used to decide between CPU and GPU\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, parameters):\n",
    "        \"\"\"Uses the given parameters to set up the Temporal Fusion Transformer model\n",
    "           \n",
    "        Args:\n",
    "          parameters: Dictionary with parameters used to define the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Inputs\n",
    "        self.col_to_idx = parameters[\"col_to_idx\"]\n",
    "        self.static_covariates = parameters[\"static_covariates\"]\n",
    "        self.category_counts = parameters[\"category_counts\"]\n",
    "\n",
    "        # Architecture\n",
    "        self.batch_size = parameters['batch_size']\n",
    "        self.hidden_size = parameters['hidden_layer_size']\n",
    "        self.dropout = parameters['dropout']\n",
    "        self.embedding_dim = parameters['embedding_dim']\n",
    "\n",
    "        # Outputs\n",
    "        self.quantiles = parameters['quantiles']\n",
    "\n",
    "        # Other\n",
    "        self.device = parameters['device']\n",
    "            \n",
    "        \n",
    "        # Prepare for input transformation (embeddings for categorical variables and linear transformations for continuous variables)\n",
    "\n",
    "        # Prepare embeddings for the static covariates and static context vectors\n",
    "        self.static_embeddings = nn.ModuleDict(\n",
    "            {col: nn.Embedding(self.category_counts[col], \n",
    "                               self.embedding_dim).to(self.device) for col in self.static_covariates}) \n",
    "        \n",
    "        self.static_variable_selection = VariableSelectionNetwork(\n",
    "            self.embedding_dim, len(self.static_covariates), self.hidden_size, self.dropout, is_temporal=False) \n",
    "\n",
    "        self.static_context_variable_selection = GatedResidualNetwork(\n",
    "            self.hidden_size, self.hidden_size, self.hidden_size, self.dropout, is_temporal=False)\n",
    "        \n",
    "        self.static_context_enrichment = GatedResidualNetwork(\n",
    "            self.hidden_size, self.hidden_size, self.hidden_size, self.dropout, is_temporal=False)\n",
    "        \n",
    "        self.static_context_state_h = GatedResidualNetwork(\n",
    "            self.hidden_size, self.hidden_size, self.hidden_size, self.dropout, is_temporal=False)\n",
    "        \n",
    "        self.static_context_state_c = GatedResidualNetwork(\n",
    "            self.hidden_size, self.hidden_size, self.hidden_size, self.dropout, is_temporal=False)\n",
    "         \n",
    "  \n",
    "\n",
    "    def define_static_covariate_encoders(self, x):\n",
    "        embedding_vectors = [\n",
    "            self.static_embeddings[col](\n",
    "                x[:, 0, self.col_to_idx[col]].long().to(self.device)) for col in self.static_covariates]\n",
    "        \n",
    "        static_embedding = torch.cat(embedding_vectors, dim=1)\n",
    "        static_encoder, static_weights = self.static_variable_selection(static_embedding)\n",
    "\n",
    "        # Static context vectors\n",
    "        static_context_s = self.static_context_variable_selection(static_encoder) # For temporal variable selection\n",
    "        static_context_e = self.static_context_enrichment(static_encoder) # For static enrichment layer\n",
    "        static_context_h = self.static_context_state_h(static_encoder) # For local processing of temporal features \n",
    "        static_context_c = self.static_context_state_c(static_encoder) # For local processing of temporal features\n",
    "\n",
    "        return static_encoder, static_weights, static_context_s, static_context_e, static_context_h, static_context_c\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Static variable selection and static covariate encoders\n",
    "        static_encoder, static_weights, static_context_s, static_context_e, static_context_h, static_context_c = self.define_static_covariate_encoders(x[\"inputs\"])\n",
    "        \n",
    "        \n",
    "        return  static_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18ca28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TemporalFusionTransformer(params)\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    break\n",
    "\n",
    "out = model.forward(batch)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c2fad1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
