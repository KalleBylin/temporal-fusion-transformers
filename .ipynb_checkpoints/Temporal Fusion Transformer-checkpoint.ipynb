{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "703731b9",
   "metadata": {},
   "source": [
    "# Temporal Fusion Transformer\n",
    "\n",
    "- Notebook author: Kalle Bylin\n",
    "\n",
    "Based on the paper *Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting* by Bryan Lima, Sercan O. ArÄ±k, Nicolas Loeff and Tomas Pfister\n",
    "\n",
    "Source: https://arxiv.org/pdf/1912.09363.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc6b940",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51da2790",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "861d036e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8efd5a2",
   "metadata": {},
   "source": [
    "## Basic Components\n",
    "\n",
    "The Temporal Fusion Transfomer architecture is composed of multiple components. We will start by building these components individually so that we then can use them on different types of problems.\n",
    "\n",
    "\n",
    "The final TFT architecture defined in the paper *Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting* has the following structure:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411c04af",
   "metadata": {},
   "source": [
    "![TFT](img/tft_architecture.png)\n",
    "\n",
    "Source: https://arxiv.org/pdf/1912.09363.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653e31d6",
   "metadata": {},
   "source": [
    "### Gated Linear Unit & Gated Residual Network\n",
    "\n",
    "Gated Residual Network blocks are among the main basic components of this network. They enable efficient information flow along with the skip connections and gating layers.\n",
    "\n",
    "The gating mechanisms basically allow the network to adapt both depth and complexity in order to perform well on a wide range of datasets and tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0cf71d",
   "metadata": {},
   "source": [
    "**Gated Linear Unit**\n",
    "\n",
    "It is hard to know which variables are actually relevant for the prediction task from the outset. The gates of the Gated Linear Unit make it possible to suppress parts of the architecture that are not necessary in a particular scneario or with a specific dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45d946a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLU(nn.Module):\n",
    "    \"\"\"\n",
    "      The Gated Linear Unit GLU(a,b) = mult(a,sigmoid(b)) is common in NLP \n",
    "      architectures like the Gated CNN. Here sigmoid(b) corresponds to a gate \n",
    "      that controls what information from a is passed to the following layer. \n",
    "\n",
    "      Args:\n",
    "          input_size (int): number defining input and output size of the gate\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input\n",
    "        self.a = nn.Linear(input_size, input_size)\n",
    "\n",
    "        # Gate\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.b = nn.Linear(input_size, input_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.tensor): tensor passing through the gate\n",
    "        \"\"\"\n",
    "        gate = self.sigmoid(self.b(x))\n",
    "        x = self.a(x)\n",
    "        \n",
    "        return torch.mul(gate, x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cd79c4",
   "metadata": {},
   "source": [
    "**Temporal Layer**\n",
    "\n",
    "Keras has a TimeDistributed layer wrapper that makes it possible to apply a layer to every temporal slice of an input.\n",
    "\n",
    "For example, it can be used to apply the same instance of a convolutional layer with the same set of weights on each timestep in the data. \n",
    "\n",
    "This TemporalLayer tries to reproduce this same functionality in Pytorch by collapsing the tensor before passing it through the layer and then rebuilding the original shape before returning the resulting tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e77669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalLayer(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Collapses input of dim T*N*H to (T*N)*H, and applies to a module.\n",
    "        Allows handling of variable sequence lengths and minibatch sizes.\n",
    "\n",
    "        Similar to TimeDistributed in Keras, it is a wrapper that makes it possible\n",
    "        to apply a layer to every temporal slice of an input.\n",
    "        \"\"\"\n",
    "        self.module = module\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.tensor): tensor with time steps to pass through the same layer.\n",
    "        \"\"\"\n",
    "        t, n = x.size(0), x.size(1)\n",
    "        x = x.reshape(t * n, -1)\n",
    "        x = self.module(x)\n",
    "        x = x.reshape(t, n, x.size(-1))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4447c1",
   "metadata": {},
   "source": [
    "**Gated Residual Network**\n",
    "\n",
    "The Gated Residual Network is a flexible block that can apply non-linear processing when required. The Gated Linear Unit defined above helps the GRN how much to contribute to its input and could potentially skip the layer altogether if necessary. GLU outputs close to 0 would supreess the non-linear contribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c8a0cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedResidualNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "      The Gated Residual Network gives the model flexibility to apply non-linear\n",
    "      processing only when needed. It is difficult to know beforehand which\n",
    "      variables are relevant and in some cases simpler models can be beneficial.\n",
    "\n",
    "      GRN(a, c) = LayerNorm(a + GLU(eta_1)) # Dropout is applied to eta_1\n",
    "        eta_1 = W_1*eta_2 + b_1\n",
    "        eta_2 = ELU(W_2*a + W_3*c + b_2)\n",
    "      \n",
    "      Args:\n",
    "          input_size (int): Size of the input\n",
    "          hidden_size (int): Size of the hidden layer\n",
    "          output_size (int): Size of the output layer\n",
    "          dropout (float): Fraction between 0 and 1 corresponding to the degree of dropout used\n",
    "          context_size (int): Size of the static context vector\n",
    "          is_temporal (bool): Flag to decide if TemporalLayer has to be used or not\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout, context_size=None, is_temporal=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.context_size = context_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "        self.is_temporal = is_temporal\n",
    "        \n",
    "        if self.is_temporal:\n",
    "            if self.input_size != self.output_size:\n",
    "                self.skip_layer = TemporalLayer(nn.Linear(self.input_size, self.output_size))\n",
    "\n",
    "            # Context vector c\n",
    "            if self.context_size != None:\n",
    "                self.c = TemporalLayer(nn.Linear(self.context_size, self.hidden_size, bias=False))\n",
    "\n",
    "            # Dense & ELU\n",
    "            self.dense1 = TemporalLayer(nn.Linear(self.input_size, self.hidden_size))\n",
    "            self.elu = nn.ELU()\n",
    "\n",
    "            # Dense & Dropout\n",
    "            self.dense2 = TemporalLayer(nn.Linear(self.hidden_size,  self.output_size))\n",
    "            self.dropout = nn.Dropout(self.dropout)\n",
    "\n",
    "            # Gate, Add & Norm\n",
    "            self.gate = TemporalLayer(GLU(self.output_size))\n",
    "            self.layer_norm = TemporalLayer(nn.BatchNorm1d(self.output_size))\n",
    "\n",
    "        else:\n",
    "            if self.input_size != self.output_size:\n",
    "                self.skip_layer = nn.Linear(self.input_size, self.output_size)\n",
    "\n",
    "            # Context vector c\n",
    "            if self.context_size != None:\n",
    "                self.c = nn.Linear(self.context_size, self.hidden_size, bias=False)\n",
    "\n",
    "            # Dense & ELU\n",
    "            self.dense1 = nn.Linear(self.input_size, self.hidden_size)\n",
    "            self.elu = nn.ELU()\n",
    "\n",
    "            # Dense & Dropout\n",
    "            self.dense2 = nn.Linear(self.hidden_size,  self.output_size)\n",
    "            self.dropout = nn.Dropout(self.dropout)\n",
    "\n",
    "            # Gate, Add & Norm\n",
    "            self.gate = GLU(self.output_size)\n",
    "            self.layer_norm = nn.BatchNorm1d(self.output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, c=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.tensor): tensor thas passes through the GRN\n",
    "            c (torch.tensor): Optional static context vector\n",
    "        \"\"\"\n",
    "\n",
    "        if self.input_size!=self.output_size:\n",
    "            a = self.skip_layer(x)\n",
    "        else:\n",
    "            a = x\n",
    "        \n",
    "        x = self.dense1(x)\n",
    "\n",
    "        if c != None:\n",
    "            c = self.c(c.unsqueeze(1))\n",
    "            x += c\n",
    "\n",
    "        eta_2 = self.elu(x)\n",
    "        \n",
    "        eta_1 = self.dense2(eta_2)\n",
    "        eta_1 = self.dropout(eta_1)\n",
    "\n",
    "        gate = self.gate(eta_1)\n",
    "        gate += a\n",
    "        x = self.layer_norm(gate)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641136f7",
   "metadata": {},
   "source": [
    "### Variable Selection Network\n",
    "\n",
    "The Variable Selection Network is a critical component of the TFT architecture. This model accepts a wide variety of inputs as can be seen in this image from the paper:\n",
    "\n",
    "![TFT inputs](img/tft_inputs.png)\n",
    "\n",
    "\n",
    "Observed inputs are time dependent variables that are known only up until the moment when we want to forecast the target variable (this includes past values of the target variable).\n",
    "\n",
    "Known inputs are time dependent variables that can be known ahead of time (e.g. holidays, special events, etc.)\n",
    "\n",
    "Static covariates can also be used to enrich the model (e.g. region of a store).\n",
    "\n",
    "With so many variables we might end up with unnecessary noise that can have a negative impact on the performance of the model. The Variable Selection Network makes it possible for the model to eliminate this noise.\n",
    "\n",
    "The Variable Selection Network can then also be used to evaluate which variables are most important for the prediction task. This is critical for interpretability of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25487825",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariableSelectionNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "      The Variable Selection Network gives the model the ability to remove\n",
    "      unnecessary noisy inputs that could have a negative impact on performance.\n",
    "      It also allows us to better understand which variables are most important\n",
    "      for the prediction task.\n",
    "\n",
    "      The variable selection weights are created by feeding both the flattened\n",
    "      vector of all past inputs at time t (E_t) and an optional context vector \n",
    "      through a GRN, followed by a Softmax layer.\n",
    "\n",
    "      V_xt = Softmax(GRN_v(E_t, c_s)) \n",
    "\n",
    "      Also, the feature vector for each variable is fed through its \n",
    "      own GRN to create an additional layer of non-linear processing.\n",
    "\n",
    "      Processed features are then weighted by the variable selection weights\n",
    "      and combined.\n",
    "\n",
    "      Args:\n",
    "          input_size (int): Size of the input\n",
    "          output_size (int): Size of the output layer\n",
    "          hidden_size (int): Size of the hidden layer\n",
    "          dropout (float): Fraction between 0 and 1 corresponding to the degree of dropout used\n",
    "          context_size (int): Size of the static context vector\n",
    "          is_temporal (bool): Flag to decide if TemporalLayer has to be used or not\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size, hidden_size, dropout, context_size=None, is_temporal=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout = dropout\n",
    "        self.context_size = context_size\n",
    "        self.is_temporal = is_temporal\n",
    "       \n",
    "        self.flattened_inputs = GatedResidualNetwork(self.output_size*self.input_size, \n",
    "                                                     self.hidden_size, self.output_size, \n",
    "                                                     self.dropout, self.context_size, \n",
    "                                                     self.is_temporal)\n",
    "        \n",
    "        self.transformed_inputs = nn.ModuleList(\n",
    "            [GatedResidualNetwork(\n",
    "                self.input_size, self.hidden_size, self.hidden_size, \n",
    "                self.dropout, self.context_size, self.is_temporal) for i in range(self.output_size)])\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "\n",
    "    def forward(self, embedding, context=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          embedding (torch.tensor): Entity embeddings for categorical variables and linear \n",
    "                     transformations for continuous variables.\n",
    "          context (torch.tensor): The context is obtained from a static covariate encoder and\n",
    "                   is naturally omitted for static variables as they already\n",
    "                   have access to this\n",
    "        \"\"\"\n",
    "\n",
    "        # Generation of variable selection weights\n",
    "        sparse_weights = self.flattened_inputs(embedding, context)\n",
    "        if self.is_temporal:\n",
    "            sparse_weights = self.softmax(sparse_weights).unsqueeze(2)\n",
    "        else:\n",
    "            sparse_weights = self.softmax(sparse_weights).unsqueeze(1)\n",
    "\n",
    "        # Additional non-linear processing for each feature vector\n",
    "        transformed_embeddings = torch.stack(\n",
    "            [self.transformed_inputs[i](embedding[\n",
    "                Ellipsis, i*self.input_size:(i+1)*self.input_size]) for i in range(self.output_size)], axis=-1)\n",
    "\n",
    "        # Processed features are weighted by their corresponding weights and combined\n",
    "        combined = transformed_embeddings*sparse_weights\n",
    "        combined = combined.sum(axis=-1)\n",
    "\n",
    "        return combined, sparse_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75f92a9",
   "metadata": {},
   "source": [
    "### Interpretable Multi-Head Attention\n",
    "\n",
    "This particular block is used to learn long-term relationships from observed time-varying inputs. It is a modified version of the more general multi-head attention block used in transformer-based architectures, in order to improve explainability.\n",
    "\n",
    "Scaled Dot-Product Attention and Multi-Head Attention were both presented in the paper \"Attention Is All You Need\" by Vaswani et al. \n",
    "\n",
    "It is well-known that the dot-product is a very simple but powerful tool to evaluate similarity between two vectors. For this same reason, it is also a great tool to help our model know what parts of the inputs to focus on based on the keys and queries. The scaling factor helps improve the performance of dot product attention by not allowing the softmax to move into regions with very small gradients. \n",
    "\n",
    "Multi-head attention allows us to compute multiple attention computations in parallel on different projections of the keys, queries and values. This makes it possible for the model to leverage different types of information in the input which would otherwise be lost by the averaging effect in a single attention head.\n",
    "\n",
    "The original version fails in allowing us to be able to interpret the importance of each feature. **The TFT proposes a modification of multi-head attention such that there are shared value weights among the different heads with an additive aggregation of the heads for better interpretability**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23245572",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention mechansims usually scale values based on relationships between\n",
    "    keys and queries. \n",
    "    \n",
    "    Attention(Q,K,V) = A(Q,K)*V where A() is a normalization function.\n",
    "\n",
    "    A common choice for the normalization function is scaled dot-product attention:\n",
    "\n",
    "    A(Q,K) = Softmax(Q*K^T / sqrt(d_attention))\n",
    "\n",
    "    Args:\n",
    "          dropout (float): Fraction between 0 and 1 corresponding to the degree of dropout used\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          query (torch.tensor): \n",
    "          key (torch.tensor):\n",
    "          value (torch.tensor): \n",
    "          mask (torch.tensor):\n",
    "        \"\"\"\n",
    "\n",
    "        d_k = key.shape[-1]\n",
    "        scaling_factor = torch.sqrt(torch.tensor(d_k).to(torch.float32))\n",
    "\n",
    "        scaled_dot_product = torch.bmm(query, key.permute(0,2,1)) / scaling_factor \n",
    "        if mask != None:\n",
    "            scaled_dot_product = scaled_dot_product.masked_fill(mask == 0, -1e9)\n",
    "        attention = self.softmax(scaled_dot_product)\n",
    "        attention = self.dropout(attention)\n",
    "\n",
    "        output = torch.bmm(attention, value)\n",
    "\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d6b02b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterpretableMultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Different attention heads can be used to improve the learning capacity of \n",
    "    the model. \n",
    "\n",
    "    MultiHead(Q,K,V) = [H_1, ..., H_m]*W_H\n",
    "    H_h = Attention(Q*Wh_Q, K*Wh_K, V*Wh_V)\n",
    "\n",
    "    Each head has specific weights for keys, queries and values. W_H linearly\n",
    "    combines the concatenated outputs from all heads.\n",
    "\n",
    "    To increase interpretability, multi-head attention has been modified to share\n",
    "    values in each head.\n",
    "\n",
    "    InterpretableMultiHead(Q,K,V) = H_I*W_H\n",
    "    H_I = 1/H * SUM(Attention(Q*Wh_Q, K*Wh_K, V*W_V)) # Note that W_V does not depend on the head. \n",
    "\n",
    "    Args:\n",
    "          num_heads (int): Number of attention heads\n",
    "          hidden_size (int): Hidden size of the model\n",
    "          dropout (float): Fraction between 0 and 1 corresponding to the degree of dropout used\n",
    "    \"\"\"\n",
    "    def __init__(self, num_attention_heads, hidden_size, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.d = hidden_size // num_attention_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.qs = nn.ModuleList([nn.Linear(self.d, self.hidden_size, bias=False) for i in range(self.num_attention_heads)])\n",
    "        self.ks = nn.ModuleList([nn.Linear(self.d, self.hidden_size, bias=False) for i in range(self.num_attention_heads)])\n",
    "\n",
    "        vs_layer = nn.Linear(self.d, self.hidden_size, bias=False) # Value is shared for improved interpretability\n",
    "        self.vs = nn.ModuleList([vs_layer for i in range(self.num_attention_heads)])\n",
    "\n",
    "        self.attention = ScaledDotProductAttention()\n",
    "        self.linear = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "\n",
    "        # First we need to reshape q, k, v for multihead attention and set batch first\n",
    "        tgt_len, batch_size, embed_dim = query.shape\n",
    "        src_len, _, _ = key.shape\n",
    "\n",
    "        head_dim = embed_dim // self.num_attention_heads\n",
    "        q = query.contiguous().view(tgt_len, batch_size * self.num_attention_heads, head_dim).transpose(0, 1)\n",
    "        k = key.contiguous().view(key.shape[0], batch_size * self.num_attention_heads, head_dim).transpose(0, 1)\n",
    "        v = value.contiguous().view(value.shape[0], batch_size * self.num_attention_heads, head_dim).transpose(0, 1)\n",
    "\n",
    "        # Now we iterate over each head to calculate outputs and attention\n",
    "        heads = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(self.num_attention_heads):\n",
    "            q_i = self.qs[i](q)\n",
    "            k_i = self.ks[i](k)\n",
    "            v_i = self.vs[i](v)\n",
    "            \n",
    "            head, attention = self.attention(q_i, k_i, v_i, mask)\n",
    "\n",
    "            # Revert to original target shape\n",
    "            head = head.transpose(0, 1).contiguous().view(tgt_len, batch_size * self.num_attention_heads, embed_dim)\n",
    "            head_dropout = self.dropout(head)\n",
    "\n",
    "            heads.append(head_dropout)\n",
    "            attentions.append(attention)\n",
    "\n",
    "        # Output the results\n",
    "        if self.num_attention_heads > 1:\n",
    "            heads = torch.stack(heads, dim=2).reshape(tgt_len, batch_size, -1, self.hidden_size)\n",
    "            outputs = torch.mean(heads, dim=2)\n",
    "        else:\n",
    "            outputs = head\n",
    "\n",
    "        attention = torch.stack(attentions, dim=2)\n",
    "        \n",
    "        outputs = self.linear(outputs)\n",
    "        outputs = self.dropout(outputs)\n",
    "\n",
    "        return outputs, attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1140879c",
   "metadata": {},
   "source": [
    "### Quantile Loss\n",
    "\n",
    "A prediction interval makes it possible to better understand the uncertainty around a particular point estimation. This makes it possible to optimize decisions and manage risk by understanding potential best and worst-case scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05dc5507",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantileLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation source: https://medium.com/the-artificial-impostor/quantile-regression-part-2-6fdbc26b2629\n",
    "    \n",
    "    Different attention heads can be used to improve the learning capacity of \n",
    "    the model. \n",
    "\n",
    "    Args:\n",
    "          quantiles (list): List of quantiles that will be used for prediction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, quantiles):\n",
    "        super().__init__()\n",
    "        self.quantiles = quantiles\n",
    "        \n",
    "    def forward(self, preds, target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "              preds (torch.tensor): Model predictions\n",
    "              target (torch.tensor): Target data\n",
    "        \"\"\"\n",
    "        assert not target.requires_grad\n",
    "        assert preds.size(0) == target.size(0)\n",
    "\n",
    "        losses = []\n",
    "        for i, q in enumerate(self.quantiles):\n",
    "            errors = target - preds[:, i]\n",
    "            losses.append(torch.max((q-1) * errors, q * errors).unsqueeze(1))\n",
    "        \n",
    "        loss = torch.mean(torch.sum(torch.cat(losses, dim=1), dim=1))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd44f7e",
   "metadata": {},
   "source": [
    "## Temporal Fusion Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da7bc128",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TemporalFusionTransformer(nn.Module):\n",
    "    \"\"\"Creates a Temporal Fusion Transformer model.\n",
    "\n",
    "    For simplicity, arguments are passed within a parameters dictionary\n",
    "\n",
    "    Args:\n",
    "        col_to_idx (dict): Maps column names to their index in input array\n",
    "        static_covariates (list): Names of static covariate variables\n",
    "        time_dependent_categorical (list): Names of time dependent categorical variables\n",
    "        time_dependent_continuous (list): Names of time dependent continuous variables\n",
    "        category_counts (dict): Maps column names to the number of categories of each categorical feature\n",
    "        known_time_dependent (list): Names of known time dependent variables \n",
    "        observed_time_dependent (list): Names of observed time dependent variables\n",
    "        batch_size (int): Batch size\n",
    "        encoder_steps (int): Fixed k time steps to look back for each prediction (also size of LSTM encoder)\n",
    "        hidden_size (int): Internal state size of different layers \n",
    "        num_lstm_layers (int): Number of LSTM layers that should be used\n",
    "        dropout (float): Fraction between 0 and 1 corresponding to the degree of dropout used\n",
    "        embedding_dim (int): Dimensionality of embeddings\n",
    "        num_attention_heads (int): Number of heads for interpretable mulit-head attention\n",
    "        quantiles (list): Quantiles used for prediction. Also defines model output size\n",
    "        device (str): Used to decide between CPU and GPU\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, parameters):\n",
    "        \"\"\"Uses the given parameters to set up the Temporal Fusion Transformer model\n",
    "           \n",
    "        Args:\n",
    "          parameters: Dictionary with parameters used to define the model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Inputs\n",
    "        self.col_to_idx = parameters[\"col_to_idx\"]\n",
    "        self.static_covariates = parameters[\"static_covariates\"]\n",
    "        self.time_dependent_categorical = parameters[\"time_dependent_categorical\"]\n",
    "        self.time_dependent_continuous = parameters[\"time_dependent_continuous\"]\n",
    "        self.category_counts = parameters[\"category_counts\"]\n",
    "        self.known_time_dependent = parameters[\"known_time_dependent\"]\n",
    "        self.observed_time_dependent = parameters[\"observed_time_dependent\"]\n",
    "        self.time_dependent = self.known_time_dependent+self.observed_time_dependent\n",
    "\n",
    "        # Architecture\n",
    "        self.batch_size = parameters['batch_size']\n",
    "        self.encoder_steps = parameters['encoder_steps']\n",
    "        self.hidden_size = parameters['hidden_layer_size']\n",
    "        self.num_lstm_layers = parameters['num_lstm_layers']\n",
    "        self.dropout = parameters['dropout']\n",
    "        self.embedding_dim = parameters['embedding_dim']\n",
    "        self.num_attention_heads = parameters['num_attention_heads']\n",
    "\n",
    "        # Outputs\n",
    "        self.quantiles = parameters['quantiles']\n",
    "\n",
    "        # Other\n",
    "        self.device = parameters['device']\n",
    "            \n",
    "        \n",
    "        # Prepare for input transformation (embeddings for categorical variables and linear transformations for continuous variables)\n",
    "\n",
    "        # Prepare embeddings for the static covariates and static context vectors\n",
    "        self.static_embeddings = nn.ModuleDict({col: nn.Embedding(self.category_counts[col], self.embedding_dim).to(self.device) for col in self.static_covariates}) \n",
    "        self.static_variable_selection = VariableSelectionNetwork(self.embedding_dim, len(self.static_covariates), self.hidden_size, self.dropout, is_temporal=False) \n",
    "\n",
    "        self.static_context_variable_selection = GatedResidualNetwork(self.hidden_size, self.hidden_size, self.hidden_size, self.dropout, is_temporal=False)\n",
    "        self.static_context_enrichment = GatedResidualNetwork(self.hidden_size, self.hidden_size, self.hidden_size, self.dropout, is_temporal=False)\n",
    "        self.static_context_state_h = GatedResidualNetwork(self.hidden_size, self.hidden_size, self.hidden_size, self.dropout, is_temporal=False)\n",
    "        self.static_context_state_c = GatedResidualNetwork(self.hidden_size, self.hidden_size, self.hidden_size, self.dropout, is_temporal=False)\n",
    "        \n",
    "        # Prepare embeddings and linear transformations for time dependent variables\n",
    "        self.temporal_cat_embeddings = nn.ModuleDict({col: TemporalLayer(nn.Embedding(self.category_counts[col], self.embedding_dim)).to(self.device) for col in self.time_dependent_categorical})\n",
    "        self.temporal_real_transformations = nn.ModuleDict({col: TemporalLayer(nn.Linear(1, self.embedding_dim)).to(self.device) for col in self.time_dependent_continuous})\n",
    "\n",
    "        # Variable selection and encoder for past inputs\n",
    "        self.past_variable_selection = VariableSelectionNetwork(self.embedding_dim, len(self.time_dependent), self.hidden_size, self.dropout, context_size=self.hidden_size)\n",
    "\n",
    "        # Variable selection and decoder for known future inputs\n",
    "        self.future_variable_selection = VariableSelectionNetwork(self.embedding_dim, len([col for col in self.time_dependent if col not in self.observed_time_dependent]), \n",
    "                                                                  self.hidden_size, self.dropout, context_size=self.hidden_size)\n",
    "\n",
    "        # LSTM encoder and decoder\n",
    "        self.lstm_encoder = nn.LSTM(input_size=self.hidden_size, hidden_size=self.hidden_size, num_layers=self.num_lstm_layers, dropout=self.dropout)\n",
    "        self.lstm_decoder = nn.LSTM(input_size=self.hidden_size, hidden_size=self.hidden_size, num_layers=self.num_lstm_layers, dropout=self.dropout)\n",
    "\n",
    "        # Gated skip connection and normalization\n",
    "        self.gated_skip_connection = TemporalLayer(GLU(self.hidden_size))\n",
    "        self.add_norm = TemporalLayer(nn.BatchNorm1d(self.hidden_size))\n",
    "\n",
    "        # Temporal Fusion Decoder\n",
    "\n",
    "        # Static enrichment layer\n",
    "        self.static_enrichment = GatedResidualNetwork(self.hidden_size, self.hidden_size, self.hidden_size, self.dropout, self.hidden_size)\n",
    "        \n",
    "        # Temporal Self-attention layer\n",
    "        self.multihead_attn = InterpretableMultiHeadAttention(self.num_attention_heads, self.hidden_size)\n",
    "        self.attention_gated_skip_connection = TemporalLayer(GLU(self.hidden_size))\n",
    "        self.attention_add_norm = TemporalLayer(nn.BatchNorm1d(self.hidden_size, self.hidden_size))\n",
    "\n",
    "        # Position-wise feed-forward layer\n",
    "        self.position_wise_feed_forward = GatedResidualNetwork(self.hidden_size, self.hidden_size, self.hidden_size, self.dropout)\n",
    "\n",
    "        # Output layer\n",
    "        self.output_gated_skip_connection = TemporalLayer(GLU(self.hidden_size))\n",
    "        self.output_add_norm = TemporalLayer(nn.BatchNorm1d(self.hidden_size, self.hidden_size))\n",
    "\n",
    "        self.output = TemporalLayer(nn.Linear(self.hidden_size, len(self.quantiles)))\n",
    "        \n",
    "  \n",
    "\n",
    "    def define_static_covariate_encoders(self, x):\n",
    "        embedding_vectors = [self.static_embeddings[col](x[:, 0, self.col_to_idx[col]].long().to(self.device)) for col in self.static_covariates]\n",
    "        static_embedding = torch.cat(embedding_vectors, dim=1)\n",
    "        static_encoder, static_weights = self.static_variable_selection(static_embedding)\n",
    "\n",
    "        # Static context vectors\n",
    "        static_context_s = self.static_context_variable_selection(static_encoder) # Context for temporal variable selection\n",
    "        static_context_e = self.static_context_enrichment(static_encoder) # Context for static enrichment layer\n",
    "        static_context_h = self.static_context_state_h(static_encoder) # Context for local processing of temporal features (encoder/decoder)\n",
    "        static_context_c = self.static_context_state_c(static_encoder) # Context for local processing of temporal features (encoder/decoder)\n",
    "\n",
    "        return static_encoder, static_weights, static_context_s, static_context_e, static_context_h, static_context_c\n",
    "\n",
    "    \n",
    "    def define_past_inputs_encoder(self, x, context):\n",
    "        embedding_vectors = torch.cat([self.temporal_cat_embeddings[col](x[:, :, self.col_to_idx[col]].long()) for col in self.time_dependent_categorical], dim=2)\n",
    "        transformation_vectors = torch.cat([self.temporal_real_transformations[col](x[:, :, self.col_to_idx[col]]) for col in self.time_dependent_continuous], dim=2)\n",
    "\n",
    "        past_inputs = torch.cat([embedding_vectors, transformation_vectors], dim=2)\n",
    "        past_encoder, past_weights = self.past_variable_selection(past_inputs, context)\n",
    "\n",
    "        return past_encoder.transpose(0, 1), past_weights\n",
    "\n",
    "\n",
    "    def define_known_future_inputs_decoder(self, x, context):\n",
    "        embedding_vectors = torch.cat([self.temporal_cat_embeddings[col](x[:, :, self.col_to_idx[col]].long()) for col in self.time_dependent_categorical if col not in self.observed_time_dependent], dim=2)\n",
    "        transformation_vectors = torch.cat([self.temporal_real_transformations[col](x[:, :, self.col_to_idx[col]]) for col in self.time_dependent_continuous if col not in self.observed_time_dependent], dim=2)\n",
    "\n",
    "        future_inputs = torch.cat([embedding_vectors, transformation_vectors], dim=2)\n",
    "        future_decoder, future_weights = self.future_variable_selection(future_inputs, context)\n",
    "\n",
    "        return future_decoder.transpose(0, 1), future_weights\n",
    "\n",
    "\n",
    "    def define_lstm_encoder(self, x, static_context_h, static_context_c):\n",
    "        output, (state_h, state_c) = self.lstm_encoder(x, (static_context_h.unsqueeze(0).repeat(self.num_lstm_layers,1,1), \n",
    "                                                           static_context_c.unsqueeze(0).repeat(self.num_lstm_layers,1,1)))\n",
    "        \n",
    "        return output, state_h, state_c\n",
    "\n",
    "\n",
    "    def define_lstm_decoder(self, x, state_h, state_c):\n",
    "        output, (_, _) = self.lstm_decoder(x, (state_h.unsqueeze(0).repeat(self.num_lstm_layers,1,1), \n",
    "                                               state_c.unsqueeze(0).repeat(self.num_lstm_layers,1,1)))\n",
    "        \n",
    "        return output\n",
    "\n",
    "    \n",
    "    def get_mask(self, attention_inputs):\n",
    "        mask = torch.cumsum(torch.eye(attention_inputs.shape[1]*self.num_attention_heads, attention_inputs.shape[0]), dim=1)\n",
    "\n",
    "        return mask.unsqueeze(2).to(self.device)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Static variable selection and static covariate encoders\n",
    "        static_encoder, static_weights, static_context_s, static_context_e, static_context_h, static_context_c = self.define_static_covariate_encoders(x[\"inputs\"])\n",
    "\n",
    "        # Past input variable selection and LSTM encoder\n",
    "        past_encoder, past_weights = self.define_past_inputs_encoder(x[\"inputs\"][:, :self.encoder_steps, :].float().to(self.device), static_context_s)\n",
    "\n",
    "        # Known future inputs variable selection and LSTM decoder\n",
    "        future_decoder, future_weights = self.define_known_future_inputs_decoder(x[\"inputs\"][:, self.encoder_steps:, :].float().to(self.device), static_context_s)\n",
    "\n",
    "        \n",
    "        # Pass output from variable selection through LSTM encoder and decoder\n",
    "        encoder_output, state_h, state_c = self.define_lstm_encoder(past_encoder, static_context_h, static_context_c)\n",
    "        decoder_output = self.define_lstm_decoder(future_decoder, static_context_h, static_context_c)\n",
    "\n",
    "        # Gated skip connection before moving into the Temporal Fusion Decoder\n",
    "        variable_selection_outputs = torch.cat([past_encoder, future_decoder], dim=0)\n",
    "        lstm_outputs = torch.cat([encoder_output, decoder_output], dim=0)\n",
    "        gated_outputs = self.gated_skip_connection(lstm_outputs)\n",
    "        temporal_feature_outputs = self.add_norm(variable_selection_outputs.add(gated_outputs))\n",
    "        temporal_feature_outputs = temporal_feature_outputs.transpose(0, 1)\n",
    "\n",
    "        # Temporal Fusion Decoder\n",
    "\n",
    "        # Static enrcihment layer\n",
    "        static_enrichment_outputs = self.static_enrichment(temporal_feature_outputs, static_context_e)\n",
    "\n",
    "        # Temporal Self-attention layer\n",
    "        mask = self.get_mask(static_enrichment_outputs)\n",
    "        multihead_outputs, multihead_attention = self.multihead_attn(static_enrichment_outputs, static_enrichment_outputs, static_enrichment_outputs, mask=mask)\n",
    "        \n",
    "        attention_gated_outputs = self.attention_gated_skip_connection(multihead_outputs)\n",
    "        attention_outputs = self.attention_add_norm(attention_gated_outputs.add(static_enrichment_outputs))\n",
    "\n",
    "        # Position-wise feed-forward layer\n",
    "        temporal_fusion_decoder_outputs = self.position_wise_feed_forward(attention_outputs)\n",
    "\n",
    "        # Output layer\n",
    "        gate_outputs = self.output_gated_skip_connection(temporal_fusion_decoder_outputs)\n",
    "        norm_outputs = self.output_add_norm(gate_outputs.add(temporal_feature_outputs))\n",
    "\n",
    "        output = self.output(norm_outputs[:, self.encoder_steps:, :]).view(-1,3)\n",
    "        \n",
    "        \n",
    "        return  output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9358d3",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d1d8d3",
   "metadata": {},
   "source": [
    "The dataset code is decoupled from our model training code for better readability and modularity.\n",
    "\n",
    "As described in Pytorch documentation, a custom Dataset class must implement three functions: \\_\\_init__, \\_\\_len__, and \\_\\_getitem__ .\n",
    "\n",
    "Source: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f2dcaf",
   "metadata": {},
   "source": [
    "### Pytorch Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "570f662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFT_Dataset(Dataset):\n",
    "    def __init__(self, data, entity_column, time_column, target_column, \n",
    "                 input_columns, encoder_steps, decoder_steps):\n",
    "        \"\"\"\n",
    "          data (pd.DataFrame): dataframe containing raw data\n",
    "          entity_column (str): name of column containing entity data\n",
    "          time_column (str): name of column containing date data\n",
    "          target_column (str): name of column we need to predict\n",
    "          input_columns (list): list of string names of columns used as input\n",
    "          encoder_steps (int): number of known past time steps used for forecast. Equivalent to size of LSTM encoder\n",
    "          decoder_steps (int): number of input time steps used for each forecast date. Equivalent to the width N of the decoder\n",
    "        \"\"\"\n",
    "        \n",
    "        self.encoder_steps = encoder_steps\n",
    "        self.transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "        \n",
    "        \n",
    "        inputs = []\n",
    "        outputs = []\n",
    "        entity = []\n",
    "        time = []\n",
    "        \n",
    "        c = 0\n",
    "        for _, entity_group in data.groupby(entity_column):\n",
    "          \n",
    "            data_time_steps = len(entity_group)\n",
    "\n",
    "            if data_time_steps >= decoder_steps:\n",
    "                x = entity_group[input_columns].copy().values\n",
    "                inputs.append(np.stack([x[i:data_time_steps - (decoder_steps - 1) + i, :] for i in range(decoder_steps)], axis=1))\n",
    "\n",
    "                y = entity_group[[target_column]].copy().values\n",
    "                outputs.append(np.stack([y[i:data_time_steps - (decoder_steps - 1) + i, :] for i in range(decoder_steps)], axis=1))\n",
    "\n",
    "                e = entity_group[[entity_column]].copy().values\n",
    "                entity.append(np.stack([e[i:data_time_steps - (decoder_steps - 1) + i, :] for i in range(decoder_steps)], axis=1))\n",
    "\n",
    "                t = entity_group[[time_column]].copy().values.astype(np.int64)\n",
    "                time.append(np.stack([t[i:data_time_steps - (decoder_steps - 1) + i, :] for i in range(decoder_steps)], axis=1))\n",
    "\n",
    "            else:\n",
    "                inputs.append(None)\n",
    "                outputs.append(None)\n",
    "                entity.append(None)\n",
    "                time.append(None)\n",
    "                \n",
    "            c+=1\n",
    "\n",
    "        self.inputs = np.concatenate(inputs, axis=0)\n",
    "        print(c, self.inputs.shape)\n",
    "        self.outputs = np.concatenate(outputs, axis=0)[:, encoder_steps:, :]\n",
    "        self.entity = np.concatenate(entity, axis=0)\n",
    "        self.time = np.concatenate(time, axis=0)\n",
    "        self.active_inputs = np.ones_like(outputs)\n",
    "\n",
    "        self.sampled_data = {\n",
    "            'inputs': self.inputs,\n",
    "            'outputs': self.outputs[:, self.encoder_steps:, :],\n",
    "            'active_entries': np.ones_like(self.outputs[:, self.encoder_steps:, :]),\n",
    "            'time': self.time,\n",
    "            'identifier': self.entity\n",
    "        }\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        s = {\n",
    "        'inputs': self.inputs[index],\n",
    "        'outputs': self.outputs[index], \n",
    "        'active_entries': np.ones_like(self.outputs[index]), \n",
    "        'time': self.time[index],\n",
    "        'identifier': self.entity[index]\n",
    "        }\n",
    "\n",
    "        return s\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.inputs.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9542bc",
   "metadata": {},
   "source": [
    "### Data preprocessing helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13bbb7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_preprocessing(train, real_columns, categorical_columns):\n",
    "    real_scalers = StandardScaler().fit(train[real_columns].values)\n",
    "\n",
    "    categorical_scalers = {}\n",
    "    num_classes = []\n",
    "    for col in categorical_columns:\n",
    "        srs = train[col].apply(str) \n",
    "        categorical_scalers[col] = LabelEncoder().fit(srs.values)\n",
    "        num_classes.append(srs.nunique())\n",
    "\n",
    "    return real_scalers, categorical_scalers\n",
    "\n",
    "\n",
    "def transform_inputs(df, real_scalers, categorical_scalers, real_columns, categorical_columns):\n",
    "    out = df.copy()\n",
    "    out[real_columns] = real_scalers.transform(df[real_columns].values)\n",
    "\n",
    "    for col in categorical_columns:\n",
    "        string_df = df[col].apply(str)\n",
    "        out[col] = categorical_scalers[col].transform(string_df)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f682dc7",
   "metadata": {},
   "source": [
    "## Toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fe4e53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebTrafficGenerator:\n",
    "    def __init__(self, start_date='2021-01-01', end_date='2024-12-31', trend_base=0.5, \n",
    "                 weekly_seasonality=None, yearly_seasonality=None, noise_multiplier=10):\n",
    "        self.dates = dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "        self.trend_base = trend_base\n",
    "        self.weekly_seasonality = weekly_seasonality\n",
    "        self.yearly_seasonality = yearly_seasonality\n",
    "        self.noise_multiplier = noise_multiplier\n",
    "        self.web_traffic = []\n",
    "        \n",
    "    def generate_data(self):\n",
    "        \n",
    "        day = 24 * 60 * 60\n",
    "        week = day * 7\n",
    "        year = 365.2425 * day\n",
    "        \n",
    "        if self.yearly_seasonality:\n",
    "            yearly = ((1 + np.sin(self.dates.view('int64') // 1e9 * \\\n",
    "                                  (self.yearly_seasonality * np.pi / year))) * 100).astype(int)\n",
    "        else:\n",
    "            yearly = 0\n",
    "            \n",
    "        if self.weekly_seasonality:\n",
    "            weekly = ((1 + np.sin(self.dates.view('int64') // 1e9 * \\\n",
    "                                  (self.weekly_seasonality * np.pi / week))) * 10).astype(int)\n",
    "        else:\n",
    "            weekly = 0\n",
    "            \n",
    "        trend = np.array(range(len(self.dates)))*self.trend_base\n",
    "        noise = ((np.random.random(len(self.dates))-0.5)*self.noise_multiplier).astype(int)\n",
    "\n",
    "        return trend+yearly+weekly+noise\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c39f40a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "traffic_generator = WebTrafficGenerator(start_date='2021-01-01', \n",
    "                                        end_date='2023-06-30',\n",
    "                                        trend_base=0.5, \n",
    "                                        weekly_seasonality=0.7,\n",
    "                                        yearly_seasonality=2.3,\n",
    "                                        noise_multiplier=80)\n",
    "traffic = traffic_generator.generate_data()\n",
    "\n",
    "plt.plot(traffic_generator.dates, traffic)\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50325e91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bc4eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25042585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5013c13",
   "metadata": {},
   "source": [
    "## Volatility data\n",
    "\n",
    "In the next lines we can see the result of running our version of the Temporal Fusion Transformer Model on one of the datasets used by the authors in the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3682814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('../formatted_omi_vol.csv', index_col=0)\n",
    "raw_data[\"date\"] = pd.to_datetime(raw_data[\"date\"])\n",
    "\n",
    "train = raw_data[raw_data['year'] < 2016]\n",
    "valid = raw_data.loc[(raw_data['year'] >= 2016) & (raw_data['year'] < 2018)]\n",
    "test = raw_data.loc[(raw_data['year'] >= 2018) & (raw_data.index <= '2019-06-28')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b2bf66f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>rv10</th>\n",
       "      <th>nobs</th>\n",
       "      <th>medrv</th>\n",
       "      <th>rk_parzen</th>\n",
       "      <th>rv5</th>\n",
       "      <th>bv_ss</th>\n",
       "      <th>rk_th2</th>\n",
       "      <th>bv</th>\n",
       "      <th>open_time</th>\n",
       "      <th>...</th>\n",
       "      <th>date</th>\n",
       "      <th>days_from_start</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>week_of_year</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>categorical_id</th>\n",
       "      <th>log_vol</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-03 00:00:00+00:00</th>\n",
       "      <td>.AEX</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>1795.0</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>90101.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>.AEX</td>\n",
       "      <td>-8.946668</td>\n",
       "      <td>EMEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-04 00:00:00+00:00</th>\n",
       "      <td>.AEX</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>1785.0</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>90416.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>.AEX</td>\n",
       "      <td>-8.510686</td>\n",
       "      <td>EMEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-05 00:00:00+00:00</th>\n",
       "      <td>.AEX</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>1801.0</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>90016.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>.AEX</td>\n",
       "      <td>-7.619135</td>\n",
       "      <td>EMEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-06 00:00:00+00:00</th>\n",
       "      <td>.AEX</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>1799.0</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.000258</td>\n",
       "      <td>90016.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2000-01-06</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>.AEX</td>\n",
       "      <td>-8.398790</td>\n",
       "      <td>EMEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-07 00:00:00+00:00</th>\n",
       "      <td>.AEX</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>1798.0</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>90046.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2000-01-07</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>.AEX</td>\n",
       "      <td>-8.885257</td>\n",
       "      <td>EMEA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Symbol      rv10    nobs     medrv  rk_parzen  \\\n",
       "2000-01-03 00:00:00+00:00   .AEX  0.000178  1795.0  0.000050   0.000179   \n",
       "2000-01-04 00:00:00+00:00   .AEX  0.000261  1785.0  0.000075   0.000423   \n",
       "2000-01-05 00:00:00+00:00   .AEX  0.000714  1801.0  0.000166   0.000324   \n",
       "2000-01-06 00:00:00+00:00   .AEX  0.000182  1799.0  0.000152   0.000219   \n",
       "2000-01-07 00:00:00+00:00   .AEX  0.000157  1798.0  0.000039   0.000155   \n",
       "\n",
       "                                rv5     bv_ss    rk_th2        bv  open_time  \\\n",
       "2000-01-03 00:00:00+00:00  0.000130  0.000100  0.000102  0.000100    90101.0   \n",
       "2000-01-04 00:00:00+00:00  0.000201  0.000207  0.000201  0.000207    90416.0   \n",
       "2000-01-05 00:00:00+00:00  0.000491  0.000361  0.000345  0.000361    90016.0   \n",
       "2000-01-06 00:00:00+00:00  0.000225  0.000258  0.000221  0.000258    90016.0   \n",
       "2000-01-07 00:00:00+00:00  0.000138  0.000130  0.000123  0.000130    90046.0   \n",
       "\n",
       "                           ...       date  days_from_start  day_of_week  \\\n",
       "2000-01-03 00:00:00+00:00  ... 2000-01-03                0            0   \n",
       "2000-01-04 00:00:00+00:00  ... 2000-01-04                1            1   \n",
       "2000-01-05 00:00:00+00:00  ... 2000-01-05                2            2   \n",
       "2000-01-06 00:00:00+00:00  ... 2000-01-06                3            3   \n",
       "2000-01-07 00:00:00+00:00  ... 2000-01-07                4            4   \n",
       "\n",
       "                           day_of_month  week_of_year  month  year  \\\n",
       "2000-01-03 00:00:00+00:00             3             1      1  2000   \n",
       "2000-01-04 00:00:00+00:00             4             1      1  2000   \n",
       "2000-01-05 00:00:00+00:00             5             1      1  2000   \n",
       "2000-01-06 00:00:00+00:00             6             1      1  2000   \n",
       "2000-01-07 00:00:00+00:00             7             1      1  2000   \n",
       "\n",
       "                           categorical_id   log_vol Region  \n",
       "2000-01-03 00:00:00+00:00            .AEX -8.946668   EMEA  \n",
       "2000-01-04 00:00:00+00:00            .AEX -8.510686   EMEA  \n",
       "2000-01-05 00:00:00+00:00            .AEX -7.619135   EMEA  \n",
       "2000-01-06 00:00:00+00:00            .AEX -8.398790   EMEA  \n",
       "2000-01-07 00:00:00+00:00            .AEX -8.885257   EMEA  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b323813c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EMEA     52919\n",
       "APAC     29655\n",
       "AMER     23417\n",
       "APAC      3891\n",
       "Name: Region, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.Region.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68740d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 109882 entries, 2000-01-03 00:00:00+00:00 to 2015-12-31 00:00:00+00:00\n",
      "Data columns (total 29 columns):\n",
      " #   Column           Non-Null Count   Dtype         \n",
      "---  ------           --------------   -----         \n",
      " 0   Symbol           109882 non-null  object        \n",
      " 1   rv10             109882 non-null  float64       \n",
      " 2   nobs             109882 non-null  float64       \n",
      " 3   medrv            109881 non-null  float64       \n",
      " 4   rk_parzen        109882 non-null  float64       \n",
      " 5   rv5              109882 non-null  float64       \n",
      " 6   bv_ss            109882 non-null  float64       \n",
      " 7   rk_th2           109882 non-null  float64       \n",
      " 8   bv               109882 non-null  float64       \n",
      " 9   open_time        109882 non-null  float64       \n",
      " 10  close_price      109882 non-null  float64       \n",
      " 11  rv5_ss           109882 non-null  float64       \n",
      " 12  rv10_ss          109882 non-null  float64       \n",
      " 13  close_time       109882 non-null  float64       \n",
      " 14  rsv_ss           109882 non-null  float64       \n",
      " 15  rk_twoscale      109882 non-null  float64       \n",
      " 16  rsv              109882 non-null  float64       \n",
      " 17  open_price       109882 non-null  float64       \n",
      " 18  open_to_close    109882 non-null  float64       \n",
      " 19  date             109882 non-null  datetime64[ns]\n",
      " 20  days_from_start  109882 non-null  int64         \n",
      " 21  day_of_week      109882 non-null  int64         \n",
      " 22  day_of_month     109882 non-null  int64         \n",
      " 23  week_of_year     109882 non-null  int64         \n",
      " 24  month            109882 non-null  int64         \n",
      " 25  year             109882 non-null  int64         \n",
      " 26  categorical_id   109882 non-null  object        \n",
      " 27  log_vol          109882 non-null  float64       \n",
      " 28  Region           109882 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(19), int64(6), object(3)\n",
      "memory usage: 25.1+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8de0aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_columns = ['log_vol', 'open_to_close', 'days_from_start']\n",
    "categorical_columns = ['Symbol', 'day_of_week', 'day_of_month', 'week_of_year', 'month', 'Region']\n",
    "\n",
    "real_scalers, categorical_scalers = fit_preprocessing(train, real_columns, categorical_columns)\n",
    "\n",
    "train = transform_inputs(train, real_scalers, categorical_scalers, real_columns, categorical_columns)\n",
    "valid = transform_inputs(valid, real_scalers, categorical_scalers, real_columns, categorical_columns)\n",
    "test = transform_inputs(test, real_scalers, categorical_scalers, real_columns, categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb3df172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "DEVICE = \"cpu\"\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10\n",
    "DROPOUT = 0.3\n",
    "LEARNING_RATE = 0.001\n",
    "ENCODER_STEPS = 252\n",
    "DECODER_STEPS = 252 + 5\n",
    "HIDDEN_LAYER_SIZE = 160\n",
    "EMBEDDING_DIMENSION = 8\n",
    "NUM_LSTM_LAYERS = 1\n",
    "NUM_ATTENTION_HEADS = 2\n",
    "QUANTILES = [0.1, 0.5, 0.9]\n",
    "\n",
    "\n",
    "# Dataset variables\n",
    "input_columns = [\"log_vol\", \"open_to_close\", \"days_from_start\", \"day_of_week\", \"day_of_month\", \"week_of_year\", \"month\", \"Region\", \"Symbol\"]\n",
    "target_column = \"log_vol\"\n",
    "entity_column = \"Symbol\"\n",
    "time_column = \"date\"\n",
    "col_to_idx = {col: idx for idx, col in enumerate(input_columns)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51dc0274",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"quantiles\": QUANTILES,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"dropout\": DROPOUT,\n",
    "    \"hidden_layer_size\": HIDDEN_LAYER_SIZE,\n",
    "    \"num_lstm_layers\": NUM_LSTM_LAYERS,\n",
    "    \"embedding_dim\": EMBEDDING_DIMENSION,\n",
    "    \"encoder_steps\": ENCODER_STEPS,\n",
    "    \"num_attention_heads\": NUM_ATTENTION_HEADS,\n",
    "    \"col_to_idx\": {col: idx for idx, col in enumerate(input_columns)},\n",
    "    \"static_covariates\": [\"Region\", \"Symbol\"],\n",
    "    \"time_dependent_categorical\": [\"day_of_week\", \"day_of_month\", \"week_of_year\", \"month\"],\n",
    "    \"time_dependent_continuous\": ['log_vol', 'days_from_start'],\n",
    "    \"category_counts\": {\"day_of_week\": 7, \"day_of_month\": 31, \"week_of_year\": 53, \"month\": 12, \"Region\": 4, \"Symbol\": 31},\n",
    "    \"known_time_dependent\": [\"day_of_week\", \"day_of_month\", \"week_of_year\", \"month\", \"days_from_start\"],\n",
    "    \"observed_time_dependent\": [\"log_vol\"],\n",
    "    \"device\": \"cpu\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11ee7adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 (101946, 257, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 (7635, 257, 9)\n",
      "31 (3532, 257, 9)\n"
     ]
    }
   ],
   "source": [
    "training_data = TFT_Dataset(train, entity_column, time_column, target_column, input_columns, ENCODER_STEPS, DECODER_STEPS)\n",
    "validation_data = TFT_Dataset(valid, entity_column, time_column, target_column, input_columns, ENCODER_STEPS, DECODER_STEPS)\n",
    "testing_data = TFT_Dataset(test, entity_column, time_column, target_column, input_columns, ENCODER_STEPS, DECODER_STEPS)\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, num_workers=2, shuffle=False)\n",
    "valid_dataloader = DataLoader(validation_data, batch_size=BATCH_SIZE, num_workers=2, shuffle=False)\n",
    "test_dataloader = DataLoader(testing_data, batch_size=BATCH_SIZE, num_workers=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82e3136",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(test_dataloader)\n",
    "batch = dataiter.next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad814e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"inputs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49ced5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kalle\\AppData\\Local\\Continuum\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model = TemporalFusionTransformer(params)\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    break\n",
    "\n",
    "out = model.forward(batch)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cddf6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = QuantileLoss(QUANTILES)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "NUM_EPOCHS = 2\n",
    "print_every_k = 10\n",
    "losses = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    t0 = time.time()\n",
    "    print(f\"===== Epoch {epoch+1} =========\")\n",
    "    epoch_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, batch in enumerate(test_dataloader):\n",
    "        labels = batch['outputs'][:,:,0].flatten().float().to(DEVICE)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(batch)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if (i+1) % print_every_k == 0:\n",
    "            print(f\"Mini-batch {i+1} average loss: {round(running_loss / print_every_k, 5)}\")\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    t1 = time.time()\n",
    "    print(f\"\\nEpoch trained for {round(t1-t0, 2)} seconds\")\n",
    "    print(\"\\nEpoch loss:\", round(epoch_loss / (i+1), 5), \"\\n\")\n",
    "    losses.append(epoch_loss / (i+1))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2047ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd8928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(valid_dataloader)\n",
    "batch = dataiter.next()\n",
    "\n",
    "dates = pd.to_datetime(batch['time'].numpy()[0,:252,0]).tolist()\n",
    "for i in range(0, 64, 5):\n",
    "    dates += pd.to_datetime(batch['time'].numpy()[i,252:,0]).tolist()\n",
    "\n",
    "known = batch[\"inputs\"].numpy()[0, :252, col_to_idx[\"log_vol\"]]\n",
    "\n",
    "labels = []\n",
    "for i in range(0, 64, 5):\n",
    "    labels += list(batch[\"outputs\"].numpy()[i, :, 0])\n",
    "\n",
    "outputs = model.forward(batch).to(\"cpu\").detach().numpy()\n",
    "outs = outputs.reshape(64,5,3)[:64:5,:,:].reshape(-1, 3)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.lineplot(x=dates[:252], y=known)\n",
    "sns.lineplot(x=dates[252:], y=labels, color=\"black\")\n",
    "sns.lineplot(x=dates[252:], y=outs[:, 0], color=\"red\")\n",
    "sns.lineplot(x=dates[252:], y=outs[:, 1], color=\"orange\")\n",
    "sns.lineplot(x=dates[252:], y=outs[:, 2], color=\"green\")\n",
    "\n",
    "plt.fill_between(dates[252:], outs[:, 0], outs[:, 2], alpha=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9f7406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
