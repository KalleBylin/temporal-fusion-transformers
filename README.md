# Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting

### Higher School of Economics
### Master of Data Science ML Project


## Papers & References

- Bryan Lim, Sercan Arik, Nicolas Loeff and Tomas Pfister. Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting. https://arxiv.org/pdf/1912.09363.pdf

- A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, I. Polosukhin, Attention is all you need, in: NIPS, 2017.

- S. S. Rangapuram, et al., Deep state space models for time series forecasting, in: NIPS, 2018.

- S. Li, et al., Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting, in: NeurIPS, 2019.


## Proposal

[Project proposal PDF](https://github.com/KalleBylin/temporal-fusion-transformers/blob/main/Proposal.pdf)

[Project Report PDF](https://github.com/KalleBylin/temporal-fusion-transformers/blob/main/ML_Project_Report.pdf)

## Description

The focus of this project is to better understand how we can deal with high-dimensional time series with multiple inputs, missing values and irregular timestamps. In this situation, the performance of the classical approaches is not satisfying, and na√Øve applications of deep learning also fail. Even the deep learning models that have shown promising results tend to "black boxes" with little insight into how to interpret the results.

## Goals

- Discover interplay between classical approaches of time series prediction and modern deep learning techniques.   
- Reproduce the model in paper "Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting" by Bryan Lim, Sercan Arik, Nicolas Loeff and Tomas Pfister.
- Test the model on generated data and compare my results on datasets used by the authors in the paper.

## Team members

- Kalle Bylin

## Reproducing Experiments

[Project still in progress]
